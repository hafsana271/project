# -*- coding: utf-8 -*-
"""Human Resorce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlnAhzfaSvLa0TfpuU3uyFyW5lCpsrwx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv('/content/Human Resource (1).csv')

data.head()

data.info()

data.count()

data.describe()

data.shape

data.isna().sum()

data['education'].unique()

import numpy as np

# Define the array of possible values
education_values = np.array(["Master's & above", "Bachelor's", 'Below Secondary'], dtype=object)

# Fill missing values in the 'education' column with random selections from the array
data['education'].fillna(np.random.choice(education_values), inplace=True)

data.isna().sum()

data.head(100)

data['previous_year_rating'].unique()

# Calculate the mode of the 'previous_year_rating' column
mode_value = data['previous_year_rating'].mode()[0]

# Fill missing values in 'previous_year_rating' with the mode
data['previous_year_rating'].fillna(mode_value, inplace=True)

data.isna().sum()

# One-Hot Encoding for categorical variables
data_encoded = pd.get_dummies(data, columns=['department', 'region', 'education', 'gender', 'recruitment_channel'], drop_first=True)

# Display the first few rows of the encoded dataset to verify the changes
data_encoded.head()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load your HR dataset
file_path = '/content/Human Resource (1).csv'  # Replace with your file path
hr_data = pd.read_csv(file_path)

# List of numerical columns to check for outliers
numerical_columns = ['age', 'no_of_trainings', 'previous_year_rating', 'length_of_service',
                     'KPIs_met >80%', 'awards_won?', 'avg_training_score','is_promoted']

# Visualizing the outliers before removal
plt.figure(figsize=(16, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 4, i)
    sns.boxplot(x=hr_data[col])
    plt.title(f"Boxplot of {col} (With Outliers)")

plt.tight_layout()
plt.show()

# Function to clip outliers using IQR method
def clip_outliers_iqr(df, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)
    return df

# Apply clipping to the numerical columns
for col in numerical_columns:
    hr_data = clip_outliers_iqr(hr_data, col)



# Function to remove outliers using the IQR method
def remove_outliers_iqr(df, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

# Apply removal to all numerical columns
for col in numerical_columns:
    hr_data = remove_outliers_iqr(hr_data, col)

# Visualize data after clipping/removing outliers
plt.figure(figsize=(16, 10))
for i, col in enumerate(numerical_columns, 1):
    plt.subplot(2, 4, i)
    sns.boxplot(x=hr_data[col])
    plt.title(f"Boxplot of {col} (After Outlier Handling)")

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load your HR dataset
file_path = '/content/Human Resource (1).csv'  # Replace with your file path
hr_data = pd.read_csv(file_path)

# Select a numerical column (e.g., 'age') to plot
column = 'age'  # You can replace this with any numerical column

# Plotting the bell curve (normal distribution) for the selected column
plt.figure(figsize=(8, 6))

# Plot histogram with KDE (Kernel Density Estimate) to show the bell curve
sns.histplot(hr_data[column], kde=True, color='blue', bins=30)

# Adding labels and title
plt.title(f'Bell Curve (Normal Distribution) for {column}', fontsize=16)
plt.xlabel(column, fontsize=12)
plt.ylabel('Density', fontsize=12)

# Show the plot
plt.show()

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Load your HR dataset
file_path = '/content/Human Resource (1).csv'  # Replace with your file path
hr_data = pd.read_csv(file_path)

# Select the numerical columns you want to scale
numerical_columns = ['age', 'no_of_trainings', 'previous_year_rating', 'length_of_service', 'avg_training_score']

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Apply the scaler to the selected numerical columns
hr_data_scaled = hr_data.copy()  # Create a copy of the original data
hr_data_scaled[numerical_columns] = scaler.fit_transform(hr_data[numerical_columns])

# Display the first few rows of the scaled dataset
print(hr_data_scaled.head())

# Optionally, save the scaled data to a new CSV file
hr_data_scaled.to_csv('hr_data_scaled.csv', index=False)

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Assuming you have X (features) and y (target) in your dataset
# For example:
# X = df.drop(columns=['target'])
# y = df['target']

# Split your data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Resample the training data
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Now X_resampled and y_resampled have balanced classes
print("Original dataset shape:", y_train.value_counts())
print("Resampled dataset shape:", pd.Series(y_resampled).value_counts())

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = data_encoded.drop(columns=['is_promoted'])  # Replace 'target_column' with your actual target column name
y = data_encoded['is_promoted']  # Replace with your actual target column name

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shapes of the resulting sets
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

data.shape

from sklearn.ensemble import RandomForestClassifier

# Initialize the model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# Classification report
print('Classification Report:')
print(classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# prompt: save this model as pickle file

import pickle

# Save the trained model to a pickle file
filename = 'model.pkl'
pickle.dump(model, open(filename, 'wb'))

import pandas as pd
import numpy as np
import pickle

# Load the trained model
with open('/content/model.pkl', 'rb') as f:
    model = pickle.load(f)

# Example of random input data
random_input = {
    'age': 35,
    'no_of_trainings': 2,
    'previous_year_rating': 4.0,
    'length_of_service': 5,
    'KPIs_met >80%': 1,
    'awards_won?': 0,
    'avg_training_score': 75,
    'department_Finance': 0,
    'department_HR': 0,
    'department_Legal': 0,
    'department_Operations': 0,
    'department_Procurement': 0,
    'department_R&D': 1,
    'department_Sales & Marketing': 0,
    'department_Technology': 0,
    'region_region_2': 0,
    'region_region_3': 0,
    'region_region_4': 1,
    'region_region_5': 0,
    'region_region_6': 0,
    'region_region_7': 0,
    'region_region_8': 0,
    'region_region_9': 0,
    'region_region_10': 0,
    'region_region_11': 0,
    'region_region_12': 0,
    'region_region_13': 0,
    'region_region_14': 0,
    'region_region_15': 0,
    'region_region_16': 0,
    'region_region_17': 0,
    'region_region_18': 0,
    'region_region_19': 0,
    'region_region_20': 0,
    'region_region_21': 0,
    'region_region_22': 0,
    'region_region_23': 0,
    'region_region_24': 0,
    'region_region_25': 0,
    'region_region_26': 0,
    'region_region_27': 0,
    'region_region_28': 0,
    'region_region_29': 0,
    'region_region_30': 0,
    'region_region_31': 0,
    'region_region_32': 0,
    'region_region_33': 0,
    'education_Master\'s & above': 1,
    'education_Below Secondary': 0,
    'gender_m': 1,
    'recruitment_channel_referred': 0,
    'recruitment_channel_sourcing': 0
}

# Convert the dictionary to a Pandas DataFrame
input_df = pd.DataFrame([random_input])

# Get the feature names used during training
training_feature_names = model.feature_names_in_

# Add missing columns to input_df and fill with 0
for feature in training_feature_names:
    if feature not in input_df.columns:
        input_df[feature] = 0  # or any other suitable default value


# Reorder the columns of input_df to match the training data
input_df = input_df[training_feature_names]

# Make a prediction
prediction = model.predict(input_df) # Changed input to input_df

import pandas as pd
import numpy as np
import pickle

# Load the trained model
with open('/content/model.pkl', 'rb') as f:
    model = pickle.load(f)

# Example of random input data
random_input = {
    'age': 35,
    'no_of_trainings': 2,
    'previous_year_rating': 4.0,
    'length_of_service': 5,
    'KPIs_met >80%': 1,
    'awards_won?': 0,
    'avg_training_score': 75,
    'department_Finance': 0,
    'department_HR': 0,
    'department_Legal': 0,
    'department_Operations': 0,
    'department_Procurement': 0,
    'department_R&D': 1,
    'department_Sales & Marketing': 0,
    'department_Technology': 0,
    'region_region_2': 0,
    'region_region_3': 0,
    'region_region_4': 1,
    'region_region_5': 0,
    'region_region_6': 0,
    'region_region_7': 0,
    'region_region_8': 0,
    'region_region_9': 0,
    'region_region_10': 0,
    'region_region_11': 0,
    'region_region_12': 0,
    'region_region_13': 0,
    'region_region_14': 0,
    'region_region_15': 0,
    'region_region_16': 0,
    'region_region_17': 0,
    'region_region_18': 0,
    'region_region_19': 0,
    'region_region_20': 0,
    'region_region_21': 0,
    'region_region_22': 0,
    'region_region_23': 0,
    'region_region_24': 0,
    'region_region_25': 0,
    'region_region_26': 0,
    'region_region_27': 0,
    'region_region_28': 0,
    'region_region_29': 0,
    'region_region_30': 0,
    'region_region_31': 0,
    'region_region_32': 0,
    'region_region_33': 0,
    'education_Master\'s & above': 1,
    'education_Below Secondary': 0,
    'gender_m': 1,
    'recruitment_channel_referred': 0,
    'recruitment_channel_sourcing': 0
}

# Convert the dictionary to a Pandas DataFrame
input_df = pd.DataFrame([random_input])

# Get the feature names used during training
training_feature_names = model.feature_names_in_

# Add missing columns to input_df and fill with 0
for feature in training_feature_names:
    if feature not in input_df.columns:
        input_df[feature] = 0  # or any other suitable default value

# Reorder the columns of input_df to match the training data
input_df = input_df[training_feature_names]

# Make a prediction
prediction = model.predict(input_df)

# Display the prediction
print(f"The predicted promotion status is: {prediction[0]}")

import pandas as pd
import pickle

# Load the trained model
with open('/content/model.pkl', 'rb') as f:
    model = pickle.load(f)

# Modified input data that might lead to a prediction of 1 (is_promoted)
modified_input = {
    'age': 30,  # Younger age might have a higher chance of promotion
    'no_of_trainings': 4,  # Fewer trainings might indicate better performance
    'previous_year_rating': 4.5,  # High rating in the previous year
    'length_of_service': 5,  # Mid-level service length
    'KPIs_met >80%': 1,  # High KPIs met percentage
    'awards_won?': 1,  # Has won awards
    'avg_training_score': 85,  # High average training score
    'department_Finance': 0,
    'department_HR': 0,
    'department_Legal': 0,
    'department_Operations': 0,
    'department_Procurement': 0,
    'department_R&D': 1,  # Belongs to the R&D department
    'department_Sales & Marketing': 0,
    'department_Technology': 0,
    'region_region_2': 0,
    'region_region_3': 0,
    'region_region_4': 1,  # Assigned to region 4
    'region_region_5': 0,
    'region_region_6': 0,
    'region_region_7': 0,
    'region_region_8': 0,
    'region_region_9': 0,
    'region_region_10': 0,
    'region_region_11': 0,
    'region_region_12': 0,
    'region_region_13': 0,
    'region_region_14': 0,
    'region_region_15': 0,
    'region_region_16': 0,
    'region_region_17': 0,
    'region_region_18': 0,
    'region_region_19': 0,
    'region_region_20': 0,
    'region_region_21': 0,
    'region_region_22': 0,
    'region_region_23': 0,
    'region_region_24': 0,
    'region_region_25': 0,
    'region_region_26': 0,
    'region_region_27': 0,
    'region_region_28': 0,
    'region_region_29': 0,
    'region_region_30': 0,
    'region_region_31': 0,
    'region_region_32': 0,
    'region_region_33': 0,
    'education_Master\'s & above': 1,  # Higher education level
    'education_Below Secondary': 0,
    'gender_m': 1,  # Male gender
    'recruitment_channel_referred': 0,
    'recruitment_channel_sourcing': 1  # Sourced recruitment channel
}

# Convert the dictionary to a Pandas DataFrame
input_df = pd.DataFrame([modified_input])

# Get the feature names used during training
training_feature_names = model.feature_names_in_

# Add missing columns to input_df and fill with 0
for feature in training_feature_names:
    if feature not in input_df.columns:
        input_df[feature] = 0  # or any other suitable default value

# Reorder the columns of input_df to match the training data
input_df = input_df[training_feature_names]

# Make a prediction
prediction = model.predict(input_df)

# Display the prediction
print(f"The predicted promotion status is: {prediction[0]}")

# prompt: save model from previous cell

# Save the trained model to a file
filename = 'trained_model.pkl'
pickle.dump(model, open(filename, 'wb'))

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5)
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean():.2f}')

# prompt: how to check accuracy

# ... (Your existing code) ...

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# ... (Rest of your code) ...

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation
print(f'Cross-Validation Scores: {cv_scores}')
print(f'Mean CV Score: {cv_scores.mean():.2f}')

# ... (Rest of your code) ...

